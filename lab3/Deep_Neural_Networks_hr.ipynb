{"cells":[{"cell_type":"markdown","metadata":{"id":"c7eaZNZhrnIO"},"source":["# Uvod #\n","\n","U ovoj lekciji ćemo vidjeti kako možemo izgraditi neuronske mreže sposobne naučiti složene vrste odnosa po kojima su duboke neuronske mreže poznate.\n","\n","Ključna ideja ovdje je *modularnost*, izgradnja složene mreže od jednostavnijih funkcionalnih jedinica. Vidjeli smo kako linearna jedinica izračunava linearnu funkciju -- sada ćemo vidjeti kako kombinirati i modificirati te pojedinačne jedinice za modeliranje složenijih odnosa.\n","\n","# Slojevi #\n","\n","Neuronske mreže obično organiziraju svoje neurone u **slojeve**. Kada skupimo linearne jedinice koje imaju zajednički skup ulaza, dobivamo **gust** sloj.\n","\n","<figure style=\"padding: 1em;\">\n","<img src=\"figs/2MA4iMV.png\" width=\"300\" alt=\"A stack of three circles in an input layer connected to two circles in a dense layer.\">\n","<figcaption style=\"textalign: center; font-style: italic\"><center>Gusti sloj dviju linearnih jedinica koje primaju dva ulaza i bias.\n","</center></figcaption>\n","</figure>\n","\n","Mogli biste zamisliti svaki sloj u neuronskoj mreži kao osobu koja izvodi neku vrstu relativno jednostavne transformacije. Kroz duboku hrpu slojeva, neuronska mreža može transformirati svoje ulaze na sve složenije načine. U dobro obučenoj neuronskoj mreži, svaki sloj je transformacija koja nas malo približava rješenju.\n","\n","<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n","<strong>Mnoge vrste slojeva</strong><br>\n","\"Sloj\" u Kerasu je vrlo općenita stvar. Sloj može biti, u biti, bilo koja vrsta <em>transformacije podataka</em>. Mnogi slojevi, kao što su <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\">konvolucijski</a> i <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN\">ponavljajućih</a> slojeva, transformiraju podatke upotrebom neurona i prvenstveno se razlikuju po obrascu veza koje tvore. Drugi se pak koriste za <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\">inženjering značajki</a> ili samo <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add\">jednostavna aritmetika</a>. Postoji cijeli svijet slojeva za otkrivanje -- <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers\">bacit oko</a>!\n","</blockquote>\n","\n","# Funkcija aktivacije #\n","\n","Međutim, pokazalo se da dva gusta sloja bez ičega između nisu ništa bolja od jednog gustog sloja samog po sebi. Gusti nas slojevi sami po sebi nikada ne mogu pomaknuti iz svijeta linija i ravnina. Ono što nam treba je nešto *nelinearno*. Ono što nam treba su aktivacijske funkcije.\n","\n","<figure style=\"padding: 1em;\">\n","<img src=\"figs/OLSUEYT.png\" width=\"400\" alt=\" \">\n","<figcaption style=\"textalign: center; font-style: italic\"><center>Bez aktivacijskih funkcija, neuronske mreže mogu naučiti samo linearne odnose. Kako bismo prilagodili krivulje, morat ćemo koristiti aktivacijske funkcije.\n","</center></figcaption>\n","</figure>\n","\n","**Aktivacijska funkcija** jednostavno je neka funkcija koju primjenjujemo na svaki od izlaza sloja (njegove *aktivacije*). Najčešća je funkcija *rectifier* $max(0, x)$.\n","\n","<figure style=\"padding: 1em;\">\n","<img src=\"figs/aeIyAlF.png\" width=\"400\" alt=\"A graph of the rectifier function. The line y=x when x>0 and y=0 when x<0, making a 'hinge' shape like '_/'.\">\n","<figcaption style=\"textalign: center; font-style: italic\"><center>\n","</center></figcaption>\n","</figure>\n","\n","Funkcija rectifier ima grafikon koji je linija s negativnim dijelom \"ispravljenim\" na nulu. Primjena funkcije na izlaze neurona stvorit će *zakrivljenje* u podacima, udaljavajući nas od jednostavnih linija.\n","\n","Kada spojimo ispravljač na linearnu jedinicu, dobivamo **ispravljenu linearnu jedinicu** ili **ReLU**. (Iz tog razloga, uobičajeno je nazvati funkciju ispravljača \"ReLU funkcija\".) Primjena ReLU aktivacije na linearnu jedinicu znači da izlaz postaje `max(0, w * x + b)`, što bismo mogli nacrtati u dijagram poput:\n","\n","<figure style=\"padding: 1em;\">\n","<img src=\"figs/eFry7Yu.png\" width=\"250\" alt=\"Diagram of a single ReLU. Like a linear unit, but instead of a '+' symbol we now have a hinge '_/'. \">\n","<figcaption style=\"textalign: center; font-style: italic\"><center>Rektificirana linearna jedinica - ReLU.\n","</center></figcaption>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"r9ahF4YkrnIP"},"source":["# Slaganje gustih slojeva #\n","\n","Sada kada imamo nešto nelinearnosti, pogledajmo kako možemo složiti slojeve da bismo dobili složene transformacije podataka.\n","\n","<figure style=\"padding: 1em;\">\n","<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/Y5iwFQZ.png\" width=\"450\" alt=\"An input layer, two hidden layers, and a final linear layer.\">\n","<figcaption style=\"textalign: center; font-style: italic\"><center>Snop gustih slojeva čini \"potpuno povezanu\" mrežu.\n","</center></figcaption>\n","</figure>\n","\n","Slojevi prije izlaznog sloja ponekad se nazivaju **skriveni** jer nikad ne vidimo izravno njihove izlaze.\n","\n","Sada primijetite da je završni (izlazni) sloj linearna jedinica (što znači da nema aktivacijske funkcije). To ovu mrežu čini prikladnom za zadatak regresije, gdje pokušavamo predvidjeti neku proizvoljnu numeričku vrijednost. Drugi zadaci (poput klasifikacije) mogu zahtijevati aktivacijsku funkciju na izlazu.\n","\n","## Izrada sekvencijalnih modela ##\n","\n","`Sekvencijalni` model koji smo koristili povezat će zajedno popis slojeva redom od prvog do zadnjeg: prvi sloj dobiva ulaz, zadnji sloj proizvodi izlaz. Ovo stvara model na gornjoj slici:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"ZexIoZVErnIP"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Perica\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]}],"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","model = keras.Sequential([\n","    # the hidden ReLU layers\n","    layers.Dense(units=4, activation='relu', input_shape=[2]),\n","    layers.Dense(units=3, activation='relu'),\n","    # the linear output layer\n","    layers.Dense(units=1),\n","])"]},{"cell_type":"markdown","metadata":{"id":"wF4DCs6krnIP"},"source":["Obavezno proslijedite sve slojeve zajedno na popisu, poput `[sloj, sloj, sloj, ...]`, umjesto kao zasebne argumente. Da biste sloju dodali funkciju aktivacije, samo dajte njezin naziv u argumentu `activation`.\n","\n","# Tvoj zadatak #\n","\n","Sada, [**kreiraj duboku neuronsku mrežu**](Deep_Neural_Networks_exercise_hr.ipynb) za skup podataka *Concrete*."]}],"metadata":{"colab":{"name":"Deep Neural Networks","provenance":[]},"jupytext":{"cell_metadata_filter":"-all","formats":"ipynb"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
